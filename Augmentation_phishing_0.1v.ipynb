{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1oIN1fbNQcMOIuNA12wr1XrxKetECkn_u","authorship_tag":"ABX9TyOF1PashSJNsrXY4iYTyViW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **This colab file is only for data Augmentation**\n","\n","---\n","\n"],"metadata":{"id":"8wHbj-Wp3Xqw"}},{"cell_type":"markdown","source":["#Augmentation of present 2459 sample size of dataset"],"metadata":{"id":"SCCTtz1sqiGp"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"z3Yg8buZqazQ","executionInfo":{"status":"ok","timestamp":1760589148654,"user_tz":-330,"elapsed":3,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}}},"outputs":[],"source":["#!pip install nlpaug\n","#!pip install pandas\n","#!pip install torch torchvision torchaudio\n","#!pip install transformers\n","\n"]},{"cell_type":"code","source":["import nltk\n","\n","# Download required resources\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger_eng')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n"],"metadata":{"id":"VwUMNo3IqlE2","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760589148828,"user_tz":-330,"elapsed":172,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}},"outputId":"d804b040-faf3-48c7-e6db-14244006d405"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["#checking original dataset size\n","\n","import pandas as pd\n","df = pd.read_csv(\"/content/drive/MyDrive/SEM_3_Project/PhishingEmailData_1.csv\", encoding='latin1')\n","print(\"Original rows:\", len(df))\n","print(\"Shape:\", df.shape)\n"],"metadata":{"id":"Tq4uIcebqlCB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760589152296,"user_tz":-330,"elapsed":3467,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}},"outputId":"d3fbb356-1959-4418-a2cb-bb5a01155424"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Original rows: 189\n","Shape: (189, 13)\n"]}]},{"cell_type":"code","source":["!pip install nlpaug"],"metadata":{"id":"Fi4zBSPFqk_g","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1760589162199,"user_tz":-330,"elapsed":9896,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}},"outputId":"8fd0ae4d-5584-4f44-f28c-36668bf8996f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nlpaug\n","  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from nlpaug) (2.0.2)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from nlpaug) (2.2.2)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from nlpaug) (2.32.4)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from nlpaug) (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown>=4.0.0->nlpaug) (3.20.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->nlpaug) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->nlpaug) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->nlpaug) (2025.10.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.8)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.15.0)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n","Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nlpaug\n","Successfully installed nlpaug-1.1.11\n"]}]},{"cell_type":"code","source":["import nlpaug.augmenter.word as naw\n","import pandas as pd\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/drive/MyDrive/SEM_3_Project/PhishingEmailData_1.csv\", encoding='latin1')\n","\n","# Create synonym augmenter\n","aug = naw.SynonymAug(aug_src='wordnet')\n","\n","# Function to augment specific text columns\n","def augment_text_columns(row):\n","    new_row = row.copy()\n","    for col in ['Email_Subject', 'Email_Content', 'Closing_Remarks']:\n","        if pd.notna(row[col]):  # Check if value is not NaN\n","            new_row[col] = aug.augment(str(row[col]))\n","    return new_row\n","\n","# Apply augmentation for all rows\n","augmented_rows = df.apply(augment_text_columns, axis=1)\n","\n","# Combine with original\n","final_df = pd.concat([df, augmented_rows])\n","\n","# Save new dataset\n","final_df.to_csv(\"emails_augmented.csv\", index=False)\n"],"metadata":{"id":"W695jLtwqk83","executionInfo":{"status":"ok","timestamp":1760589208515,"user_tz":-330,"elapsed":18046,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["###check the augmented new dataset below"],"metadata":{"id":"MDDnxGIOqsZT"}},{"cell_type":"code","source":["import pandas as pd\n","df_new = pd.read_csv(\"emails_augmented.csv\")\n","print(df_new.shape)\n","print(df_new.head(10))\n"],"metadata":{"id":"GSvgR3_eqq7n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760589214224,"user_tz":-330,"elapsed":37,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}},"outputId":"40168a59-1939-4345-894d-818b3873b77f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["(378, 13)\n","                                       Email_Subject  \\\n","0                                     URGENT REQUEST   \n","1                                     Quick question   \n","2   ******Part time home work assistant needed******   \n","3                                   √ä vendor payment   \n","4                                     Quick question   \n","5  √äXXX has shared a document on Google Docs with...   \n","6                       Message from human resources   \n","7                                    Library Account   \n","8                                  Your Dropbox File   \n","9                                                NaN   \n","\n","                                       Email_Content Sending_Date  \\\n","0  Are you available ?\\nNo calls text only 951307...       1/9/20   \n","1  I'm in a meeting and need help getting some Am...       1/9/20   \n","2  Hello RECIPIENT\\n\\nI am urgently seeking for a...     10/19/19   \n","3  Are you around? I need to pay a vendor with th...  12/27/18\\n    \n","4  I'm in a meeting and need help getting some Am...  12/27/18\\n    \n","5  XXX has invited you to view the following docu...   5/3/2017\\n   \n","6  Dear XXXXX@berkeley.edu(link sends e-mail)\\n\\n...      4/13/17   \n","7  Dear Student,\\n\\nYour access to your library a...       4/1/17   \n","8  Hello,\\n \\nYou just received a file through Dr...      1/30/17   \n","9  Dear User,\\n\\nThis message is to inform you th...      1/25/17   \n","\n","     Sending_Time   Day     URL_Title                  Coined.Word  \\\n","0              na    na            na                       Urgent   \n","1              na    na            na                        Quick   \n","2         2:22 PM   Sat            na                   Job/Needed   \n","3              na    na            na           gift card, meeting   \n","4              na    na            na                       shared   \n","5              na    na  Open in Docs                invite, share   \n","6  9:29:54 PM PDT    na    click here                           HR   \n","7         2:09 PM   Sat     plain url  expiring soon, inactivity,    \n","8              na   Mon     view file                file, dropbox   \n","9              na    na           URL      expire, login, reactive   \n","\n","  Sender_Name                                       Sender_Title  \\\n","0           Y      Chancellor\\nBerkeley University of California   \n","1           Y                 University of California, Berkeley   \n","2           Y  *Professor David Card*\\n*Department of Economi...   \n","3           Y                 University of California, Berkeley   \n","4           Y                 University of California, Berkeley   \n","5          na                                                NaN   \n","6           Y  Berkeley University Of California HR Departmen...   \n","7           Y  University Library\\nUniversity of California B...   \n","8           Y                                   The Dropbox Team   \n","9           Y  Berkeley Security\\nUniversity of California, B...   \n","\n","     Closing_Remarks                           Sender_Email      Logo   To  \n","0       BEST REGARDS           cchristberkeley.edu@gmail.com       na   NB  \n","1                 na              XXX.subdomain.berkeley.edu       na   NB  \n","2        Sincerely                     dvdmson @ gmail . Com       na   NB  \n","3                 na  √ä√äXXX.subdomain.berkeley.edu@gmail.com       na   NB  \n","4                 na              XXX.subdomain.berkeley.edu       na  NaN  \n","5                 na                       √äXXX@berkeley.edu       na  BCC  \n","6          Thank you                         HR@berkeley.edu       na   NB  \n","7         Sincerely,                                      na       na   NB  \n","8  Happy Dropboxing.      sass@tamhsc.edu(link sends e-mail)  Dropbox    B  \n","9         Sincerely,                    dapatel@berkeley.edu       na    B  \n"]}]},{"cell_type":"code","source":["#checking size of dataset\n","# Show number of rows\n","print(\"Number of rows:\", len(df))\n","\n","# Or using shape (rows, columns)\n","print(\"Shape of dataset:\", df.shape)"],"metadata":{"id":"jFUfJ1LAqq5b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760589260548,"user_tz":-330,"elapsed":39,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}},"outputId":"af235d49-f4c6-44a9-cf36-c6122429d9ce"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of rows: 189\n","Shape of dataset: (189, 13)\n"]}]},{"cell_type":"code","source":["augmented_rows = df.apply(augment_text_columns, axis=1)\n"],"metadata":{"id":"ke4yNloSqq3K","executionInfo":{"status":"ok","timestamp":1760589262503,"user_tz":-330,"elapsed":559,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["failed in augmentation as our base size was small 189 and yet after its same now we are trying again with another method"],"metadata":{"id":"Ivogq1NdqypG"}},{"cell_type":"code","source":["import nlpaug.augmenter.word as naw\n","import pandas as pd\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/drive/MyDrive/SEM_3_Project/PhishingEmailData_1.csv\", encoding='latin1')\n","print(\"Original dataset size:\", df.shape)\n","\n","# Synonym augmenter\n","aug = naw.SynonymAug(aug_src='wordnet')\n","\n","# Create augmented dataset\n","augmented_rows = []\n","\n","for i, row in df.iterrows():\n","    new_row = row.copy()\n","    for col in ['Email_Subject', 'Email_Content', 'Closing_Remarks']:\n","        if pd.notna(row[col]):\n","            new_row[col] = aug.augment(str(row[col]))\n","    augmented_rows.append(new_row)\n","\n","# Convert augmented rows to dataframe\n","df_aug = pd.DataFrame(augmented_rows)\n","\n","# Combine original + augmented\n","final_df = pd.concat([df, df_aug], ignore_index=True)\n","\n","print(\"Final dataset size:\", final_df.shape)\n","\n","# Save\n","final_df.to_csv(\"emails_augmented.csv\", index=False)\n"],"metadata":{"id":"CqhfY7qkqq0G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760589264194,"user_tz":-330,"elapsed":508,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}},"outputId":"e7ba2989-1c4c-4843-b579-d37b193b618d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Original dataset size: (189, 13)\n","Final dataset size: (378, 13)\n"]}]},{"cell_type":"markdown","source":["fine the data got double but our target is to have dataset more than 4000 size for that we do"],"metadata":{"id":"wIcTC5Slq2eg"}},{"cell_type":"code","source":["import nlpaug.augmenter.word as naw\n","import pandas as pd\n","\n","# Load your dataset\n","df = pd.read_csv(\"/content/drive/MyDrive/SEM_3_Project/PhishingEmailData_1.csv\", encoding='latin1')\n","\n","# Initialize augmenter (synonym replacement)\n","aug = naw.SynonymAug(aug_src='wordnet')\n","\n","# Desired dataset size\n","target_size = 5000\n","\n","# Copy original\n","augmented_df = df.copy()\n","\n","# Keep augmenting until we reach the target\n","while len(augmented_df) < target_size:\n","    new_data = df.copy()\n","\n","    # Apply augmentation on text column (replace 'email_text' with your actual column)\n","    new_data['Email_Subject'] = new_data['Email_Subject'].apply(lambda x: aug.augment(str(x)))\n","    new_data['Email_Content'] = new_data['Email_Content'].apply(lambda x: aug.augment(str(x)))\n","    new_data['Closing_Remarks'] = new_data['Closing_Remarks'].apply(lambda x: aug.augment(str(x)))\n","\n","    augmented_df = pd.concat([augmented_df, new_data], ignore_index=True)\n","\n","# Trim extra rows if it exceeds 5000\n","augmented_df = augmented_df.head(target_size)\n","\n","print(\"Original dataset size:\", df.shape)\n","print(\"Final dataset size:\", augmented_df.shape)\n","\n","# Save final dataset\n","augmented_df.to_csv(\"augmented_dataset.csv\", index=False)\n"],"metadata":{"id":"UgEp5wXOqqxH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760589309566,"user_tz":-330,"elapsed":6110,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}},"outputId":"f265c980-e636-41a0-e951-9901d35d50e8"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Original dataset size: (189, 13)\n","Final dataset size: (5000, 13)\n"]}]},{"cell_type":"markdown","source":["Based on above result ourprevious data frm 189 rows and 13 columns we augmented same dataset to** 5000 rows of data and 13 columns**\n","\n","\n","---\n","\n","\n","check the path of the file down üëá"],"metadata":{"id":"olZHeI2eq58f"}},{"cell_type":"code","source":["import os\n","\n","file_path = \"augmented_dataset.csv\"\n","print(f\"The augmented dataset is saved at: {os.path.abspath(file_path)}\")"],"metadata":{"id":"bcUvgpvnqk6N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760589341490,"user_tz":-330,"elapsed":15,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}},"outputId":"d11812cc-4b1a-4b3c-a34d-3ac956471f2d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["The augmented dataset is saved at: /content/augmented_dataset.csv\n"]}]},{"cell_type":"markdown","source":["download the file into PC üëá"],"metadata":{"id":"uh_L7R-1q9lG"}},{"cell_type":"code","source":["from google.colab import files\n","\n","files.download('augmented_dataset.csv')"],"metadata":{"id":"GNUOGraoqk3n","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1760589343283,"user_tz":-330,"elapsed":45,"user":{"displayName":"K.S.N","userId":"07782968521927388551"}},"outputId":"06a61eb4-decd-4c8b-9850-846c06cf56e0"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_11663b46-0f77-4826-a44d-0bbe38cc8a1c\", \"augmented_dataset.csv\", 1141414)"]},"metadata":{}}]},{"cell_type":"markdown","source":["#now since our data is around 5000 we can perform"],"metadata":{"id":"M6MvCiszrA3d"}},{"cell_type":"markdown","source":["#END HERE\n","\n","\n","---\n","\n"],"metadata":{"id":"KOKiDXU53uhH"}}]}